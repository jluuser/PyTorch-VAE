data:
  train_manifest: "/public/home/zhangyangroup/chengshiz/keyuan.zhou/PyTorch-VAE/prior/out_prior_token64_K1024_D512_data/train/manifest.jsonl"
  val_manifest:   "/public/home/zhangyangroup/chengshiz/keyuan.zhou/PyTorch-VAE/prior/out_prior_token64_K1024_D512_data/val/manifest.jsonl"
  pad_token_id: null
  bos_token_id: null
  eos_token_id: null
  max_len: 256                 # flattened latent length = 64 * 4
  batch_size: 256
  num_workers: 16
  bucket_boundaries: []        # no bucketing for fixed-length

vq:
  codebook_size: 4096          # global vocab size after flattening RVQ

model:
  d_model: 512
  n_layers: 8
  n_heads: 8
  ffw_mult: 4
  dropout: 0.1
  tie_embeddings: true
  layer_norm_eps: 1e-5

optim:
  lr: 1.2e-3
  betas: [0.9, 0.95]
  weight_decay: 0.01
  warmup_updates: 1000
  max_updates: 15000
  grad_clip_norm: 1.0
  label_smoothing: 0.0

runtime:
  seed: 42
  log_interval: 100
  save_interval_updates: 10000
  eval_interval_updates: 500
  grad_accum_steps: 2
  ckpt_dir: "/public/home/zhangyangroup/chengshiz/keyuan.zhou/PyTorch-VAE/prior/prior_ckpts/prior_1_8.pt"
  device: "cuda"
  dtype: "float32"
  amp: true
  periodic_save_every: 0
  keep_last_periodic: 2