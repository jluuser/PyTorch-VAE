model_params:
  name: "VQVAE"
  input_dim: 6
  hidden_dim: 512
  num_layers: 4
  num_heads: 8
  max_seq_len: 350
  codebook_size: 256            # was 1024; denser usage is easier to stabilize
  code_dim: 64
  beta: 0.55                    # stronger commitment for EMA VQ
  history_drop_prob: 0.3
  zero_ss_in_encoder: true
  label_smoothing: 0.05
  ss_tv_lambda: 0.03
  usage_entropy_lambda: 0.002   # encourage broader code usage
  # EMA VQ recycling knobs
  reinit_dead_codes: true
  reinit_prob: 1.0
  dead_usage_threshold: 0

data_params:
  npy_dir: "/public/home/zhangyangroup/chengshiz/keyuan.zhou/prp-dataset/filtered_curves_npy/"
  train_list: "train_list.txt"
  val_list: "val_list.txt"
  mean_xyz: [59.75617548672732, 57.48255340071842, 67.46183179563933]
  std_xyz: [114.53544376107905, 120.74113448738537, 118.6619735740254]
  train_batch_size: 256
  val_batch_size: 256
  num_workers: 4
  pin_memory: true

exp_params:
  LR: 0.0002
  ss_weight: 1.2               # temporarily emphasize SS; later tune to 0.6â€“0.8
  weight_decay: 0.001
  bond_length_weight: 0.1
  bond_angle_weight: 0.05
  manual_seed: 1265

trainer_params:
  gpus: 4
  max_epochs: 120
  strategy: "ddp"
  accelerator: "gpu"
  precision: 32
  log_every_n_steps: 20
  num_sanity_val_steps: 0
  benchmark: true
  enable_progress_bar: true

logging_params:
  save_dir: "./logs/"
  name: "VQVAE"
