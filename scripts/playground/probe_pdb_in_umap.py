#!/usr/bin/env python3
# coding: utf-8

import os
import sys
import argparse
import subprocess
import tempfile
import shutil
from pathlib import Path

import numpy as np
import torch
import joblib  # [Added] Required to load the UMAP model

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

"""
Probe one or more PDBs and place them onto the UMAP embedding
using the pre-trained UMAP reducer (transform), not interpolation.

Requirements:
- Cache .npz contains 'umap_2d' (background points).
- A corresponding .pkl file exists containing the trained UMAP reducer.

Example:

python scripts/probe_pdb_in_umap.py \
  --pdb /public/home/zhangyangroup/chengshiz/run/20251107_ccx-binder-fig/ccx-binder-fig/data/GPR4-RFD100-chainA \
        /public/home/zhangyangroup/chengshiz/run/20251107_ccx-binder-fig/ccx-binder-fig/data/GPR4-RFDbeta-chainA \
        /public/home/zhangyangroup/chengshiz/run/20251107_ccx-binder-fig/ccx-binder-fig/data/GPR4-RFD1000-chainA \
        /public/home/zhangyangroup/chengshiz/run/20251107_ccx-binder-fig/ccx-binder-fig/data/GPR4-RFD3000-chainA
"""

# ----------------------------------------------------------------------
# Hard-coded paths and config (edit these to match your environment)
# ----------------------------------------------------------------------

THIS_DIR = Path(__file__).resolve().parent
REPO_ROOT = THIS_DIR.parent
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

PRP_ENV_PREFIX = Path(
    "/public/home/zhangyangroup/chengshiz/run/20250717_prp-data/prp-data/.pixi/envs/default"
)

CKPT_PATH = REPO_ROOT / "checkpoints" / "vq_s_gradient_ckpt_test11_15" / "epochepoch=549.ckpt"

# [Modified] The .npz file now contains 'umap_2d' inside it
CACHE_PATH = REPO_ROOT / "latent_analysis" / "class1" / "tsne_cache_class1_len_between_1_80.npz"

# [Added] Path to the saved UMAP model (.pkl) generated by the previous script
# Ensure this filename matches exactly what visualize_latent_and_codebook2.py outputted.
UMAP_MODEL_PATH = REPO_ROOT / "latent_analysis" / "class1" / "umap_reducer_class1_len_between_1_80.pkl"

OUTPUT_DIR = Path(
    "/public/home/zhangyangroup/chengshiz/run/20251107_ccx-binder-fig/ccx-binder-fig/examples/bb-crv_sp-probe-res/umap"
)

HIDDEN_DIM = 512
NUM_LAYERS = 4
NUM_HEADS = 8
MAX_SEQ_LEN = 350
CODE_DIM = 128
LATENT_N_TOKENS = 48

USE_AMP = True
# KNN_K = 10  <-- Removed, not needed for UMAP transform
PRP_WORKERS = 16

# ----------------------------------------------------------------------
# Imports from repo
# ----------------------------------------------------------------------

from models.vq_vae import VQVAE
from dataset import CurveDataset, pad_collate

# ----------------------------------------------------------------------
# Generic helpers
# ----------------------------------------------------------------------

def strip_prefixes(state_dict, prefixes=("model.", "module.", "net.")):
    out = {}
    for k, v in state_dict.items():
        name = k
        for p in prefixes:
            if name.startswith(p):
                name = name[len(p):]
                break
        out[name] = v
    return out


def drop_quantizer_keys(state_dict):
    keys = [k for k in state_dict.keys() if k.startswith("quantizer.")]
    for k in keys:
        state_dict.pop(k, None)
    return state_dict

# [Removed] knn_interpolate_2d is no longer needed

# ----------------------------------------------------------------------
# prp-data processing
# ----------------------------------------------------------------------

def run_prp_process_multi_pdb(pdb_files, tmp_root: Path, workers: int = PRP_WORKERS):
    """
    Run prp-data once on a directory containing all PDB files.
    Returns mapping: Path(pdb_file) -> Path(curve_npy_file).
    """
    env_prefix = PRP_ENV_PREFIX.resolve()

    pdb_input_dir = tmp_root / "pdb_input"
    curves_out_dir = tmp_root / "curves_out"
    pdb_input_dir.mkdir(parents=True, exist_ok=True)
    curves_out_dir.mkdir(parents=True, exist_ok=True)

    mapping = {}
    used_names = set()

    for idx, pdb_path in enumerate(pdb_files):
        pdb_path = pdb_path.resolve()
        base = pdb_path.name
        dest_name = base
        if dest_name in used_names:
            dest_name = "{:04d}__{}".format(idx, base)
        used_names.add(dest_name)

        dst_pdb = pdb_input_dir / dest_name
        if pdb_path != dst_pdb:
            shutil.copy2(str(pdb_path), str(dst_pdb))

        dest_stem = dst_pdb.stem
        mapping[pdb_path] = {"stem": dest_stem}

    cmd = [
        "mamba",
        "run",
        "-p",
        str(env_prefix),
        "prp-data",
        "process",
        "--input",
        str(pdb_input_dir),
        "--output",
        str(curves_out_dir),
        "--workers",
        str(int(workers)),
        "--device",
        "cpu",
        "--metadata",
        "probe_metadata.json",
    ]
    print("[PRP] Running:", " ".join(cmd))
    result = subprocess.run(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    )

    if result.returncode != 0:
        print("[PRP] stdout:\n", result.stdout)
        print("[PRP] stderr:\n", result.stderr)
        raise RuntimeError("prp-data process failed with code {}".format(result.returncode))

    print("[PRP] process finished. stdout:\n", result.stdout)

    npy_files = [p for p in curves_out_dir.iterdir() if p.suffix == ".npy"]
    if not npy_files:
        raise RuntimeError("No .npy produced by prp-data under {}".format(curves_out_dir))

    npy_files_sorted = sorted(npy_files, key=lambda x: x.name)

    pdb_to_npy = {}
    for pdb_path, meta in mapping.items():
        stem = meta["stem"]
        candidate = curves_out_dir / (stem + ".npy")

        if candidate.is_file():
            pdb_to_npy[pdb_path] = candidate
            continue

        found = None
        for fn in npy_files_sorted:
            if fn.name.startswith(stem):
                found = fn
                break

        if found is None and npy_files_sorted:
            found = npy_files_sorted[0]

        if found is None:
            raise RuntimeError(
                "Could not find any .npy for PDB {} (stem={})".format(str(pdb_path), stem)
            )

        pdb_to_npy[pdb_path] = found

    print("[PRP] Resolved {} PDBs to curve npy files".format(len(pdb_to_npy)))
    return pdb_to_npy


# ----------------------------------------------------------------------
# Encode curve to latent
# ----------------------------------------------------------------------

@torch.no_grad()
def encode_single_batch_to_latent(model, x, mask, device, use_amp: bool):
    """Encode one curve batch into sequence-level latent vector."""
    x = x.to(device, non_blocking=True)
    mask = mask.to(device, non_blocking=True)

    try:
        autocast_ctx = torch.amp.autocast(
            device_type="cuda",
            enabled=(use_amp and device.type == "cuda"),
        )
    except Exception:
        autocast_ctx = torch.cuda.amp.autocast(
            enabled=(use_amp and device.type == "cuda")
        )

    model.eval()
    with autocast_ctx:
        h_fuse, _, _ = model.encode(x, mask=mask)
        z_tok = model._tokenize_to_codes(h_fuse, mask)
        z_seq = z_tok.mean(dim=1)

    z = z_seq.cpu().numpy()[0]
    return z.astype(np.float32, copy=False)


# ----------------------------------------------------------------------
# PDB file collection
# ----------------------------------------------------------------------

def collect_pdb_files_and_groups(pdb_args):
    """
    Collect PDB files and assign a group name for each top-level argument.
    For each argument:
      - if directory: group name = directory name, all .pdb inside
      - if file: group name = file stem (only if suffix is .pdb)
    """
    pdb_files = []
    pdb_groups = []

    for arg in pdb_args:
        root = Path(arg).resolve()
        if root.is_dir():
            group_name = root.name
            for fn in sorted(root.iterdir()):
                if fn.is_file() and fn.suffix.lower() == ".pdb":
                    pdb_files.append(fn)
                    pdb_groups.append(group_name)
        elif root.is_file():
            if root.suffix.lower() == ".pdb":
                group_name = root.stem
                pdb_files.append(root)
                pdb_groups.append(group_name)
            else:
                print("[Warn] Skip non-pdb file:", str(root))
        else:
            print("[Warn] Path not found, skip:", str(root))

    if not pdb_files:
        raise RuntimeError("No valid .pdb files found from: {}".format(pdb_args))

    return pdb_files, pdb_groups


def derive_group_name(pdb_args, pdb_files):
    """Name prefix for output PNGs."""
    if len(pdb_args) == 1:
        only = Path(pdb_args[0]).resolve()
        if only.is_dir():
            return only.name
        else:
            return only.stem
    else:
        return "multi_{}_pdbs".format(len(pdb_files))


# ----------------------------------------------------------------------
# Plotting on UMAP
# ----------------------------------------------------------------------

def plot_umap_with_probes(
    base_2d,
    query_coords,
    query_labels,
    query_groups,
    out_png,
    title,
    unique_groups,
    marker_by_group,
    color_by_group,
    groups_to_show=None,
    show_labels=False,
):
    """Draw gray background plus probes for selected groups."""
    fig, ax = plt.subplots(figsize=(10.0, 10.0), dpi=220)

    ax.scatter(
        base_2d[:, 0],
        base_2d[:, 1],
        s=4,
        c="#d1d5db",
        alpha=0.7,
        edgecolors="none",
    )

    if groups_to_show is None:
        groups_to_show = unique_groups

    visible_coords = []
    visible_labels = []
    visible_groups = []

    for coord, lab, grp in zip(query_coords, query_labels, query_groups):
        if grp not in groups_to_show:
            continue
        visible_coords.append(coord)
        visible_labels.append(lab)
        visible_groups.append(grp)

    for coord, lab, grp in zip(visible_coords, visible_labels, visible_groups):
        qx, qy = float(coord[0]), float(coord[1])
        ax.scatter(
            [qx],
            [qy],
            s=50,
            c=color_by_group.get(grp, "yellow"),
            edgecolors="none",
            marker=marker_by_group.get(grp, "*"),
            linewidths=0.8,
            zorder=10,
        )

    ax.set_xlabel("UMAP dim-1")
    ax.set_ylabel("UMAP dim-2")
    ax.set_title(title)

    from matplotlib.lines import Line2D
    group_handles = []
    for g in groups_to_show:
        group_handles.append(
            Line2D(
                [0],
                [0],
                marker=marker_by_group.get(g, "*"),
                linestyle="None",
                markerfacecolor=color_by_group.get(g, "yellow"),
                markeredgecolor="none",
                markersize=6,
                label=g,
            )
        )
    ax.legend(handles=group_handles, title="Probe groups", loc="best")

    if show_labels:
        for coord, lab, grp in zip(visible_coords, visible_labels, visible_groups):
            qx, qy = float(coord[0]), float(coord[1])
            ax.text(
                qx + 0.5,
                qy + 0.5,
                lab,
                fontsize=7,
                color="black",
                weight="bold",
                zorder=11,
                clip_on=False,
            )

    fig.tight_layout()
    fig.savefig(str(out_png))
    plt.close(fig)


# ----------------------------------------------------------------------
# Args
# ----------------------------------------------------------------------

def parse_args():
    p = argparse.ArgumentParser("Probe one or more PDBs in latent UMAP space")
    p.add_argument(
        "--pdb",
        type=str,
        nargs="+",
        required=True,
        help="One or more PDB paths; each can be a file or a directory containing .pdb files",
    )
    return p.parse_args()


# ----------------------------------------------------------------------
# Main
# ----------------------------------------------------------------------

def main():
    args = parse_args()

    if not CKPT_PATH.is_file():
        raise FileNotFoundError("CKPT not found: {}".format(str(CKPT_PATH)))

    if not CACHE_PATH.is_file():
        raise FileNotFoundError("Cache .npz not found: {}".format(str(CACHE_PATH)))
    
    if not UMAP_MODEL_PATH.is_file():
        raise FileNotFoundError("UMAP model .pkl not found: {}. Please run visualize_latent_and_codebook2.py first.".format(str(UMAP_MODEL_PATH)))

    pdb_files, pdb_groups = collect_pdb_files_and_groups(args.pdb)
    group_name = derive_group_name(args.pdb, pdb_files)

    out_dir = OUTPUT_DIR
    out_dir.mkdir(parents=True, exist_ok=True)

    combined_png = (out_dir / (group_name + "_umap_all_groups.png")).resolve()

    np.random.seed(42)
    torch.manual_seed(42)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("[Device] Using:", device)
    print("[Probe] Total PDB files:", len(pdb_files))

    # [Modified] Load 'umap_2d' from the .npz cache directly
    cache = np.load(str(CACHE_PATH), allow_pickle=True)
    latents = cache["latents"] # still needed if we want to check dimensions, but UMAP transform doesn't need old latents
    
    if "umap_2d" not in cache:
        raise KeyError("Cache .npz does not contain 'umap_2d'. Did you run the updated visualize_latent_and_codebook2.py?")
    
    umap_2d = cache["umap_2d"]

    print(
        "[Cache] Loaded latents: {} x {}, UMAP: {} x {}".format(
            latents.shape[0], latents.shape[1], umap_2d.shape[0], umap_2d.shape[1]
        )
    )

    # [Added] Load the UMAP reducer model
    print("[Load] Loading UMAP model from {}".format(str(UMAP_MODEL_PATH)))
    reducer = joblib.load(str(UMAP_MODEL_PATH))

    model = VQVAE(
        input_dim=6,
        hidden_dim=HIDDEN_DIM,
        num_layers=NUM_LAYERS,
        num_heads=NUM_HEADS,
        max_seq_len=MAX_SEQ_LEN,
        use_vq=False,
        codebook_size=1,
        code_dim=CODE_DIM,
        label_smoothing=0.0,
        ss_tv_lambda=0.0,
        usage_entropy_lambda=0.0,
        xyz_align_alpha=0.7,
        dist_lambda=0.0,
        rigid_aug_prob=0.0,
        pairwise_sample_k=32,
        noise_warmup_steps=0,
        max_noise_std=0.0,
        reinit_dead_codes=False,
        reinit_prob=0.0,
        dead_usage_threshold=0,
        codebook_init_path="",
        latent_tokens=int(LATENT_N_TOKENS),
        tokenizer_heads=NUM_HEADS,
        tokenizer_layers=2,
        tokenizer_dropout=0.1,
        print_init=False,
    ).to(device)

    ckpt = torch.load(str(CKPT_PATH), map_location="cpu")
    state = ckpt.get("state_dict", ckpt)
    state = strip_prefixes(state)
    state = drop_quantizer_keys(state)
    missing, unexpected = model.load_state_dict(state, strict=False)
    print("[Load] missing={} unexpected={}".format(len(missing), len(unexpected)))

    query_coords = []
    query_labels = []
    query_groups = []

    from torch.utils.data import DataLoader

    with tempfile.TemporaryDirectory(prefix="probe_pdbs_umap_") as tmp_root_str:
        tmp_root = Path(tmp_root_str).resolve()

        pdb_to_npy = run_prp_process_multi_pdb(pdb_files, tmp_root, workers=PRP_WORKERS)

        for idx, (pdb_path, group_name_i) in enumerate(zip(pdb_files, pdb_groups)):
            pdb_name = pdb_path.name
            curve_npy_path = pdb_to_npy[pdb_path]
            npy_dir = curve_npy_path.parent
            npy_base = curve_npy_path.name

            print(
                "\n[Probe] {} / {}: {} (group: {}) (curve npy: {})".format(
                    idx + 1,
                    len(pdb_files),
                    pdb_name,
                    group_name_i,
                    str(curve_npy_path),
                )
            )

            list_path = tmp_root / ("probe_list_{}.txt".format(idx))
            with list_path.open("w") as f:
                f.write(npy_base + "\n")

            ds = CurveDataset(
                npy_dir=str(npy_dir),
                list_path=str(list_path),
                train=False,
            )
            loader = DataLoader(
                ds,
                batch_size=1,
                shuffle=False,
                num_workers=0,
                collate_fn=pad_collate,
                drop_last=False,
            )

            batch = next(iter(loader))
            if isinstance(batch, (list, tuple)):
                x, mask = batch
            else:
                x, mask = batch, None
                mask = torch.ones((x.size(0), x.size(1)), dtype=torch.bool)

            print(
                "[Probe] Loaded curve: x shape={}, mask shape={}".format(
                    tuple(x.shape), tuple(mask.shape)
                )
            )

            z_query = encode_single_batch_to_latent(
                model=model,
                x=x,
                mask=mask,
                device=device,
                use_amp=bool(USE_AMP),
            )
            print(
                "[Probe] Latent dim={}, first 5 values {}".format(
                    z_query.shape[0], z_query[:5]
                )
            )

            # [Modified] Use UMAP reducer to transform the new point
            # Reshape z_query from (128,) to (1, 128)
            z_query_reshaped = z_query.reshape(1, -1)
            query_2d_array = reducer.transform(z_query_reshaped)
            query_2d = query_2d_array[0] # Get the (x, y) coordinate

            print(
                "[Probe] UMAP transform coord: ({:.3f}, {:.3f})".format(
                    float(query_2d[0]), float(query_2d[1])
                )
            )

            query_coords.append(query_2d)
            query_labels.append(pdb_name)
            query_groups.append(group_name_i)

    # Prepare group-wise plotting
    unique_groups = []
    for g in query_groups:
        if g not in unique_groups:
            unique_groups.append(g)

    marker_cycle = ["*", "X", "o", "s", "D", "^", "v", "P"]
    color_cycle = ["#f97316", "#0ea5e9", "#a855f7", "#22c55e", "#e11d48"]

    marker_by_group = {}
    color_by_group = {}
    for idx, g in enumerate(unique_groups):
        marker_by_group[g] = marker_cycle[idx % len(marker_cycle)]
        color_by_group[g] = color_cycle[idx % len(color_cycle)]

    # 1) Combined figure: all groups on one UMAP
    if len(pdb_files) == 1:
        title_all = "UMAP background with probe: {}".format(query_labels[0])
    else:
        title_all = "UMAP background with {} probes".format(len(pdb_files))

    plot_umap_with_probes(
        base_2d=umap_2d,
        query_coords=query_coords,
        query_labels=query_labels,
        query_groups=query_groups,
        out_png=combined_png,
        title=title_all,
        unique_groups=unique_groups,
        marker_by_group=marker_by_group,
        color_by_group=color_by_group,
        groups_to_show=None,
        show_labels=False,
    )

    # 2-4) Per-group figures: each directory separately
    per_group_pngs = []
    for g in unique_groups:
        safe_g = g.replace(os.sep, "_")
        out_png_g = (out_dir / (group_name + "_umap_" + safe_g + ".png")).resolve()
        title_g = "UMAP background with probes from group: {}".format(g)

        plot_umap_with_probes(
            base_2d=umap_2d,
            query_coords=query_coords,
            query_labels=query_labels,
            query_groups=query_groups,
            out_png=out_png_g,
            title=title_g,
            unique_groups=unique_groups,
            marker_by_group=marker_by_group,
            color_by_group=color_by_group,
            groups_to_show=[g],
            show_labels=False,
        )
        per_group_pngs.append(out_png_g)

    print("\n[Done] Saved UMAP probe figures:")
    print("  all groups :", str(combined_png))
    for p in per_group_pngs:
        print("  per group  :", str(p))


if __name__ == "__main__":
    main()